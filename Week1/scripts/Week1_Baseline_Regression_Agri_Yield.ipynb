{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d0f305",
   "metadata": {},
   "source": [
    "# Week 1 — Vision & Foundation: Baseline Regression (Agricultural Yield)\n",
    "DataVerse Africa Internship Cohort 3.0 — Data Science Track\n",
    "\n",
    "**What you’ll do**: Frame an ML problem, explore data, build a leak-free preprocessing pipeline, and train a baseline regression model. \n",
    "\n",
    "**Deliverable**: A baseline model + short research brief. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c972a9bc",
   "metadata": {},
   "source": [
    "\n",
    "## Learning Outcomes\n",
    "- Explain supervised vs. unsupervised learning and where regression fits.\n",
    "- Run an end‑to‑end *tabular* ML workflow with scikit‑learn.\n",
    "- Perform basic EDA (shape, missingness, distributions, correlations).\n",
    "- Build a **ColumnTransformer + Pipeline** with `SimpleImputer`, `StandardScaler`, and `OneHotEncoder`.\n",
    "- Train/evaluate baseline regressors (Linear Regression, Random Forest) using **MAE/RMSE/R²**.\n",
    "- Avoid **data leakage** using pipelines and proper splits.\n",
    "\n",
    "> **Tip**: This notebook uses a **synthetic Nigeria‑like crop yield dataset** to make the lab self‑contained. In project weeks, swap in FAOSTAT/field data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4df34e",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "Installed libraries assumed: `numpy`, `pandas`, `matplotlib`, `scikit-learn`, `joblib` (optional).\n",
    "\n",
    "> If you get import errors, install locally (e.g., `pip install pandas scikit-learn matplotlib joblib`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76802e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import joblib, os, pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d02131a",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Load data\n",
    "We'll start with a small, tabular dataset: synthetic crop yields (tons/ha) with climate and management features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd38b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_PATH = r\"/mnt/data/week1_synthetic_agri_yield.csv\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33b499",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Quick EDA (keep it *question‑driven*)\n",
    "**Guiding questions**  \n",
    "1. What is the target distribution? (Range/plausibility)  \n",
    "2. Which features are numerical vs categorical? Any missingness?  \n",
    "3. Do simple bivariate plots hint at relationships worth modeling?  \n",
    "\n",
    "> We'll restrict to a few plots to keep Week‑1 time‑boxed. Avoid overfitting your story to EDA! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4aa886",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.info()\n",
    "print(\"\\nSummary stats:\")\n",
    "display(df.describe(include='all').T)\n",
    "\n",
    "print(\"\\nMissingness (%):\")\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False) * 100\n",
    "display(missing_pct)\n",
    "\n",
    "# Hist of target\n",
    "plt.figure()\n",
    "df['yield_t_ha'].hist(bins=30)\n",
    "plt.title('Yield (t/ha)')\n",
    "\n",
    "# Scatter: rainfall vs yield\n",
    "plt.figure()\n",
    "plt.scatter(df['rainfall_mm'], df['yield_t_ha'], alpha=0.5)\n",
    "plt.xlabel('rainfall_mm'); plt.ylabel('yield_t_ha'); plt.title('Rainfall vs Yield')\n",
    "\n",
    "# Boxplot: region vs yield\n",
    "plt.figure()\n",
    "# simple matplotlib boxplot expects sequences\n",
    "grouped = [df.loc[df['region']==r, 'yield_t_ha'].dropna() for r in df['region'].dropna().unique()]\n",
    "plt.boxplot(grouped, labels=df['region'].dropna().unique(), vert=True, showmeans=True)\n",
    "plt.xticks(rotation=30)\n",
    "plt.title('Yield by Region')\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d486e44c",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Train/test split and preprocessing (leak‑safe)\n",
    "- Split **before** any data‑dependent transforms.\n",
    "- Use `ColumnTransformer` to combine numeric and categorical pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf29970",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target = 'yield_t_ha'\n",
    "features = [c for c in df.columns if c != target]\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', numeric_pipe, numeric_features),\n",
    "    ('cat', categorical_pipe, categorical_features)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b1f59",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Baseline models\n",
    "Start simple: Linear Regression and Random Forest. Use identical preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9549a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(model, X_train, y_train, X_test, y_test, name='model'):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    rmse = mean_squared_error(y_test, preds, squared=False)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"{name}: MAE={mae:.3f}, RMSE={rmse:.3f}, R^2={r2:.3f}\")\n",
    "    return {'model': name, 'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "linreg = Pipeline([('prep', preprocess), ('model', LinearRegression())])\n",
    "rf = Pipeline([('prep', preprocess), ('model', RandomForestRegressor(random_state=42, n_estimators=300))])\n",
    "\n",
    "scores = []\n",
    "scores.append(evaluate(linreg, X_train, y_train, X_test, y_test, 'LinearRegression'))\n",
    "scores.append(evaluate(rf, X_train, y_train, X_test, y_test, 'RandomForestRegressor'))\n",
    "\n",
    "pd.DataFrame(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1218c970",
   "metadata": {},
   "source": [
    "\n",
    "### Notes & next steps\n",
    "- **Pick the metric** that matches your business objective (MAE often reads in the same units as the target).  \n",
    "- For Week‑2 we'll add **cross‑validation** and **hyperparameter tuning**; for now record a **baseline**.\n",
    "- Save the best baseline for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ff584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best = max(scores, key=lambda d: d['R2'])\n",
    "best_name = best['model']\n",
    "best_pipe = rf if best_name == 'RandomForestRegressor' else linreg\n",
    "out_path = pathlib.Path('best_baseline_model.joblib')\n",
    "joblib.dump(best_pipe, out_path)\n",
    "print('Saved:', out_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de83e32",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: Optional classification demo (for Tuesday live-coding)\n",
    "This short example demonstrates binary classification using Logistic Regression with scikit‑learn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356df8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "Xc = data['data']; yc = data['target']\n",
    "\n",
    "clf = make_pipeline(\n",
    "    SimpleImputer(strategy='median'),\n",
    "    StandardScaler(with_mean=False),\n",
    "    LogisticRegression(max_iter=200)\n",
    ")\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, yc, test_size=0.2, random_state=42)\n",
    "clf.fit(Xc_train, yc_train)\n",
    "print('Classification R^2 doesn\\'t apply; use accuracy:')\n",
    "print('Accuracy:', (clf.score(Xc_test, yc_test)))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
