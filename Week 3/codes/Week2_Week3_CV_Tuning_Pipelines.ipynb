{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edb79195",
   "metadata": {},
   "source": [
    "# Test cell — notebook will be updated next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933eb3ae",
   "metadata": {},
   "source": [
    "# Week 2–3: Cross-Validation, Hyperparameter Tuning, Ensembles, SHAP & Pipelines\n",
    "\n",
    "**Audience:** Beginner → Intermediate  \n",
    "**Focus:** Practical, scikit-learn-first implementations (with optional XGBoost + SHAP)  \n",
    "**Datasets:** `sklearn.datasets` (Iris, Breast Cancer), plus small synthetic examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131165e",
   "metadata": {},
   "source": [
    "---\n",
    "## What you'll learn\n",
    "1. Cross-validation: K-Fold, Stratified K-Fold, Leave-One-Out\n",
    "2. Hyperparameter tuning: `GridSearchCV`, `RandomizedSearchCV`\n",
    "3. Ensembles: RandomForest (+ optional XGBoost with early stopping)\n",
    "4. Model interpretability with SHAP (for tree models)\n",
    "5. Clean end-to-end ML Pipelines (`Pipeline`, `ColumnTransformer`) with hyperparameter search\n",
    "\n",
    "> Tip: Run each section independently. Some optional dependencies (xgboost, shap) may need installation in your environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f86c2d",
   "metadata": {},
   "source": [
    "## 0) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2417e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_iris, load_breast_cancer, make_classification\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, KFold, \n",
    "                                     StratifiedKFold, LeaveOneOut, GridSearchCV, RandomizedSearchCV)\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score,\n",
    "                             confusion_matrix, classification_report, roc_auc_score)\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Optional deps: xgboost & shap\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "    print(\"XGBoost not available in this environment. You can install with: pip install xgboost\")\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "    print(\"SHAP not available in this environment. You can install with: pip install shap\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4817f996",
   "metadata": {},
   "source": [
    "## 1) Cross-Validation: K-Fold, Stratified, Leave-One-Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d3163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iris (multiclass)\n",
    "X_iris, y_iris = load_iris(return_X_y=True)\n",
    "clf = LogisticRegression(max_iter=200)\n",
    "\n",
    "print(\"K-Fold (k=5) on Iris\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_kf = cross_val_score(clf, X_iris, y_iris, cv=kf, scoring='accuracy')\n",
    "print(\"Scores:\", np.round(scores_kf, 3), \"Mean:\", scores_kf.mean().round(3))\n",
    "\n",
    "print(\"\\nStratified K-Fold (k=5) on Iris\")\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores_skf = cross_val_score(clf, X_iris, y_iris, cv=skf, scoring='accuracy')\n",
    "print(\"Scores:\", np.round(scores_skf, 3), \"Mean:\", scores_skf.mean().round(3))\n",
    "\n",
    "print(\"\\nLeave-One-Out (LOOCV) on Iris (this can be slow)\")\n",
    "loo = LeaveOneOut()\n",
    "scores_loo = cross_val_score(clf, X_iris, y_iris, cv=loo, scoring='accuracy')\n",
    "print(\"Mean LOOCV Accuracy:\", scores_loo.mean().round(3))\n",
    "\n",
    "# Breast Cancer (binary) with stratified CV\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
    "clf_bc = LogisticRegression(max_iter=1000)\n",
    "print(\"\\nStratified K-Fold on Breast Cancer (binary)\")\n",
    "scores_bc = cross_val_score(clf_bc, X_bc, y_bc, cv=skf, scoring='accuracy')\n",
    "print(\"Scores:\", np.round(scores_bc, 3), \"Mean:\", scores_bc.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad636d",
   "metadata": {},
   "source": [
    "## 2) Hyperparameter Tuning: GridSearchCV & RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea553d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import randint\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bc, y_bc, test_size=0.2, stratify=y_bc, random_state=42)\n",
    "\n",
    "# A) Logistic Regression: GridSearch over C\n",
    "param_grid_logit = {'C': [0.01, 0.1, 1, 10, 100], 'solver': ['lbfgs']}\n",
    "grid_logit = GridSearchCV(LogisticRegression(max_iter=1000), param_grid_logit, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_logit.fit(X_train, y_train)\n",
    "print(\"LogisticRegression best params:\", grid_logit.best_params_)\n",
    "print(\"CV best score:\", round(grid_logit.best_score_, 3))\n",
    "print(\"Test accuracy:\", round(grid_logit.best_estimator_.score(X_test, y_test), 3))\n",
    "\n",
    "# B) Decision Tree: GridSearch on depth / min_samples_split\n",
    "param_grid_tree = {'max_depth': [None, 2, 4, 6, 8], 'min_samples_split': [2, 5, 10]}\n",
    "grid_tree = GridSearchCV(DecisionTreeClassifier(random_state=0), param_grid_tree, cv=5, scoring='f1', n_jobs=-1)\n",
    "grid_tree.fit(X_train, y_train)\n",
    "print(\"\\nDecisionTree best params:\", grid_tree.best_params_)\n",
    "print(\"CV best F1:\", round(grid_tree.best_score_, 3))\n",
    "y_pred_tree = grid_tree.best_estimator_.predict(X_test)\n",
    "print(\"Test F1:\", round(f1_score(y_test, y_pred_tree), 3))\n",
    "\n",
    "# C) Random Forest: RandomizedSearch\n",
    "param_dist_rf = {\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "rand_rf = RandomizedSearchCV(RandomForestClassifier(random_state=42),\n",
    "                             param_distributions=param_dist_rf,\n",
    "                             n_iter=15, cv=5, scoring='roc_auc',\n",
    "                             random_state=42, n_jobs=-1)\n",
    "rand_rf.fit(X_train, y_train)\n",
    "print(\"\\nRandomForest best params:\", rand_rf.best_params_)\n",
    "print(\"CV best ROC AUC:\", round(rand_rf.best_score_, 3))\n",
    "y_proba_rf = rand_rf.best_estimator_.predict_proba(X_test)[:,1]\n",
    "print(\"Test ROC AUC:\", round(roc_auc_score(y_test, y_proba_rf), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4908a1e",
   "metadata": {},
   "source": [
    "## 3) Pipelines + ColumnTransformer (+ tuning inside a Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64c3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mixed-type mini dataset\n",
    "df = pd.DataFrame({\n",
    "    'age': [22, 35, 58, 44, 29, 63, 51, 40, 33, 27],\n",
    "    'balance': [1200, 5600, 23000, 11000, 3400, 54000, 16000, 9800, 7400, 2500],\n",
    "    'region': ['A','B','A','C','B','C','A','B','C','A'],\n",
    "    'is_vip': ['no','no','yes','no','no','yes','yes','no','no','no'],\n",
    "    'churn': [0,1,1,0,0,1,0,1,0,0]\n",
    "})\n",
    "\n",
    "numeric_features = ['age', 'balance']\n",
    "categorical_features = ['region', 'is_vip']\n",
    "\n",
    "numeric_pipeline = Pipeline([('scaler', StandardScaler()),\n",
    "                             ('pca', PCA(n_components=1))])\n",
    "categorical_pipeline = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_features),\n",
    "    ('cat', categorical_pipeline, categorical_features)\n",
    "])\n",
    "\n",
    "pipe = Pipeline([('preprocess', preprocess),\n",
    "                 ('clf', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "X_df = df.drop(columns=['churn']); y_df = df['churn']\n",
    "\n",
    "param_grid_pipe = {\n",
    "    'preprocess__num__pca__n_components': [1, 2],\n",
    "    'clf__max_depth': [None, 3, 5],\n",
    "    'clf__n_estimators': [50, 100]\n",
    "}\n",
    "grid_pipe = GridSearchCV(pipe, param_grid_pipe, cv=3, scoring='accuracy')\n",
    "grid_pipe.fit(X_df, y_df)\n",
    "print(\"Best pipeline params:\", grid_pipe.best_params_)\n",
    "print(\"CV best accuracy:\", round(grid_pipe.best_score_, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03745a59",
   "metadata": {},
   "source": [
    "## 4) (Optional) XGBoost with Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c65a797",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'HAS_XGB' in globals() and HAS_XGB:\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(load_breast_cancer(return_X_y=True)[0],\n",
    "                                                load_breast_cancer(return_X_y=True)[1],\n",
    "                                                test_size=0.2, stratify=load_breast_cancer(return_X_y=True)[1], random_state=42)\n",
    "    xgb_clf = xgb.XGBClassifier(\n",
    "        n_estimators=300, learning_rate=0.05, max_depth=3,\n",
    "        subsample=0.9, colsample_bytree=0.9,\n",
    "        objective='binary:logistic', eval_metric='logloss', random_state=42\n",
    "    )\n",
    "    xgb_clf.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], early_stopping_rounds=20, verbose=False)\n",
    "    print(\"Best iteration (trees):\", getattr(xgb_clf, \"best_iteration\", None))\n",
    "    y_pred = xgb_clf.predict(X_val)\n",
    "    print(\"XGBoost validation accuracy:\", round(accuracy_score(y_val, y_pred), 3))\n",
    "else:\n",
    "    print(\"XGBoost skipped (package not installed).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade924a",
   "metadata": {},
   "source": [
    "## 5) Model Interpretability with SHAP (summary plot + single-prediction force plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b4dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit a RandomForest for interpretability demo\n",
    "X_bc, y_bc = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bc, y_bc, test_size=0.2, stratify=y_bc, random_state=42)\n",
    "rf_int = RandomForestClassifier(n_estimators=200, random_state=0).fit(X_train, y_train)\n",
    "\n",
    "if 'HAS_SHAP' in globals() and HAS_SHAP:\n",
    "    explainer = shap.Explainer(rf_int, X_train)   # TreeExplainer for RF\n",
    "    shap_values = explainer(X_test)\n",
    "    shap.summary_plot(shap_values.values, X_test, feature_names=load_breast_cancer().feature_names)\n",
    "    shap.initjs()\n",
    "    display(shap.plots.force(shap_values[0]))\n",
    "else:\n",
    "    print(\"SHAP skipped (package not installed).\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
