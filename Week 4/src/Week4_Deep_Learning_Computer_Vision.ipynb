{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "main-title",
   "metadata": {},
   "source": [
    "# Week 4: Deep Learning & Computer Vision\n",
    "## Crop Disease Detection System - Vision & Foundation\n",
    "\n",
    "**DataVerse Africa Internship Cohort 3.0 · Data Science**\n",
    "\n",
    "---\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this week, you will:\n",
    "1. Understand the fundamentals of neural networks and backpropagation\n",
    "2. Learn OpenCV basics for image preprocessing\n",
    "3. Explore and analyze image datasets systematically\n",
    "4. Design preprocessing pipelines for computer vision tasks\n",
    "5. Build your first CNN for crop disease detection\n",
    "\n",
    "### Dataset: Grapevine Leaves Classification\n",
    "- **5 classes**: Ak, Ala_Idris, Buzgulu, Dimnit, Nazli\n",
    "- **500 images total** (100 per class)\n",
    "- **Task**: Multi-class image classification\n",
    "- **Application**: Agricultural crop variety identification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports-setup",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "OpenCV version: 4.12.0\n",
      "GPU Available: []\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Computer Vision\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"OpenCV version: {cv2.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-networks-theory",
   "metadata": {},
   "source": [
    "## 2. Neural Networks Basics & Theory\n",
    "\n",
    "### 2.1 What are Neural Networks?\n",
    "\n",
    "Neural networks are computational models inspired by biological neural networks. They consist of:\n",
    "- **Neurons (nodes)**: Basic processing units\n",
    "- **Weights**: Connection strengths between neurons\n",
    "- **Biases**: Threshold adjustments\n",
    "- **Activation functions**: Non-linear transformations\n",
    "\n",
    "### 2.2 Key Concepts for Computer Vision\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are specifically designed for image data:\n",
    "- **Convolution layers**: Extract features using filters/kernels\n",
    "- **Pooling layers**: Reduce spatial dimensions\n",
    "- **Fully connected layers**: Final classification\n",
    "\n",
    "**Why CNNs for Images?**\n",
    "1. **Translation invariance**: Features detected regardless of position\n",
    "2. **Parameter sharing**: Same filter applied across entire image\n",
    "3. **Hierarchical learning**: Low-level → High-level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "simple-neural-network",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │            \u001b[38;5;34m16\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m10\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> (104.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26\u001b[0m (104.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> (104.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26\u001b[0m (104.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    }
   ],
   "source": [
    "# Simple neural network visualization\n",
    "def visualize_simple_nn():\n",
    "    \"\"\"\n",
    "    Demonstrate a simple neural network structure\n",
    "    \"\"\"\n",
    "    # Create a simple 2-layer network\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(4, activation='relu', input_shape=(3,), name='hidden_layer'),\n",
    "        layers.Dense(2, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    # Display model architecture\n",
    "    model.summary()\n",
    "    \n",
    "    # Visualize with keras plot_model (if available)\n",
    "    try:\n",
    "        keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)\n",
    "    except:\n",
    "        print(\"Model visualization requires graphviz. Architecture shown above.\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "simple_model = visualize_simple_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opencv-basics",
   "metadata": {},
   "source": [
    "## 3. OpenCV Basics for Image Preprocessing\n",
    "\n",
    "OpenCV (Open Source Computer Vision Library) is essential for image preprocessing in computer vision projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opencv-demo",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display a sample image\n",
    "dataset_path = Path('../Grapevine_Leaves_Image_Dataset')\n",
    "sample_image_path = dataset_path / 'Ak' / 'Ak (1).png'\n",
    "\n",
    "# Read image using OpenCV\n",
    "img_cv = cv2.imread(str(sample_image_path))\n",
    "img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "\n",
    "# Read image using PIL\n",
    "img_pil = Image.open(sample_image_path)\n",
    "\n",
    "# Display both\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axes[0].imshow(img_rgb)\n",
    "axes[0].set_title(f'OpenCV Image\\nShape: {img_rgb.shape}')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(img_pil)\n",
    "axes[1].set_title(f'PIL Image\\nSize: {img_pil.size}')\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Image shape (OpenCV): {img_rgb.shape}\")\n",
    "print(f\"Image size (PIL): {img_pil.size}\")\n",
    "print(f\"Data type: {img_rgb.dtype}\")\n",
    "print(f\"Value range: {img_rgb.min()} - {img_rgb.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-preprocessing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_image_preprocessing(image_path):\n",
    "    \"\"\"\n",
    "    Demonstrate common image preprocessing techniques\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    img = cv2.imread(str(image_path))\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # 1. Resize\n",
    "    img_resized = cv2.resize(img_rgb, (224, 224))\n",
    "    \n",
    "    # 2. Grayscale conversion\n",
    "    img_gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 3. Gaussian blur\n",
    "    img_blurred = cv2.GaussianBlur(img_rgb, (15, 15), 0)\n",
    "    \n",
    "    # 4. Edge detection\n",
    "    edges = cv2.Canny(img_gray, 100, 200)\n",
    "    \n",
    "    # 5. Histogram equalization\n",
    "    img_gray_eq = cv2.equalizeHist(img_gray)\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    axes[0, 0].imshow(img_rgb)\n",
    "    axes[0, 0].set_title('Original')\n",
    "    axes[0, 0].axis('off')\n",
    "    \n",
    "    axes[0, 1].imshow(img_resized)\n",
    "    axes[0, 1].set_title('Resized (224x224)')\n",
    "    axes[0, 1].axis('off')\n",
    "    \n",
    "    axes[0, 2].imshow(img_gray, cmap='gray')\n",
    "    axes[0, 2].set_title('Grayscale')\n",
    "    axes[0, 2].axis('off')\n",
    "    \n",
    "    axes[1, 0].imshow(img_blurred)\n",
    "    axes[1, 0].set_title('Gaussian Blur')\n",
    "    axes[1, 0].axis('off')\n",
    "    \n",
    "    axes[1, 1].imshow(edges, cmap='gray')\n",
    "    axes[1, 1].set_title('Edge Detection')\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    axes[1, 2].imshow(img_gray_eq, cmap='gray')\n",
    "    axes[1, 2].set_title('Histogram Equalization')\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demonstrate preprocessing\n",
    "demonstrate_image_preprocessing(sample_image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dataset-exploration",
   "metadata": {},
   "source": [
    "## 4. Dataset Exploration & Analysis\n",
    "\n",
    "Systematic dataset exploration is crucial for understanding your data and designing appropriate preprocessing pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_dataset_structure(dataset_path):\n",
    "    \"\"\"\n",
    "    Explore the structure and statistics of the image dataset\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Get class directories\n",
    "    class_dirs = [d for d in dataset_path.iterdir() if d.is_dir()]\n",
    "    class_names = [d.name for d in class_dirs]\n",
    "    \n",
    "    print(f\"Dataset: {dataset_path.name}\")\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "    # Count images per class\n",
    "    class_counts = {}\n",
    "    total_images = 0\n",
    "    \n",
    "    for class_dir in class_dirs:\n",
    "        image_files = list(class_dir.glob('*.png')) + list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.jpeg'))\n",
    "        count = len(image_files)\n",
    "        class_counts[class_dir.name] = count\n",
    "        total_images += count\n",
    "        print(f\"{class_dir.name}: {count} images\")\n",
    "    \n",
    "    print(f\"\\nTotal images: {total_images}\")\n",
    "    \n",
    "    # Visualize class distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(class_counts.keys(), class_counts.values())\n",
    "    plt.title('Class Distribution')\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Number of Images')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_names, class_counts\n",
    "\n",
    "# Explore our dataset\n",
    "class_names, class_counts = explore_dataset_structure(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "image-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_properties(dataset_path, class_names, sample_size=10):\n",
    "    \"\"\"\n",
    "    Analyze image properties like dimensions, channels, file sizes\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    image_stats = {\n",
    "        'widths': [],\n",
    "        'heights': [],\n",
    "        'channels': [],\n",
    "        'file_sizes': [],\n",
    "        'classes': []\n",
    "    }\n",
    "    \n",
    "    # Sample images from each class\n",
    "    for class_name in class_names:\n",
    "        class_dir = dataset_path / class_name\n",
    "        image_files = list(class_dir.glob('*.png'))[:sample_size]\n",
    "        \n",
    "        for img_path in image_files:\n",
    "            # Load image\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is not None:\n",
    "                h, w, c = img.shape\n",
    "                file_size = img_path.stat().st_size / 1024  # KB\n",
    "                \n",
    "                image_stats['heights'].append(h)\n",
    "                image_stats['widths'].append(w)\n",
    "                image_stats['channels'].append(c)\n",
    "                image_stats['file_sizes'].append(file_size)\n",
    "                image_stats['classes'].append(class_name)\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    df_stats = pd.DataFrame(image_stats)\n",
    "    \n",
    "    print(\"Image Statistics Summary:\")\n",
    "    print(df_stats.describe())\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Width distribution\n",
    "    axes[0, 0].hist(df_stats['widths'], bins=20, alpha=0.7)\n",
    "    axes[0, 0].set_title('Width Distribution')\n",
    "    axes[0, 0].set_xlabel('Width (pixels)')\n",
    "    \n",
    "    # Height distribution\n",
    "    axes[0, 1].hist(df_stats['heights'], bins=20, alpha=0.7)\n",
    "    axes[0, 1].set_title('Height Distribution')\n",
    "    axes[0, 1].set_xlabel('Height (pixels)')\n",
    "    \n",
    "    # File size distribution\n",
    "    axes[1, 0].hist(df_stats['file_sizes'], bins=20, alpha=0.7)\n",
    "    axes[1, 0].set_title('File Size Distribution')\n",
    "    axes[1, 0].set_xlabel('File Size (KB)')\n",
    "    \n",
    "    # Aspect ratio\n",
    "    aspect_ratios = df_stats['widths'] / df_stats['heights']\n",
    "    axes[1, 1].hist(aspect_ratios, bins=20, alpha=0.7)\n",
    "    axes[1, 1].set_title('Aspect Ratio Distribution')\n",
    "    axes[1, 1].set_xlabel('Width/Height Ratio')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df_stats\n",
    "\n",
    "# Analyze image properties\n",
    "image_stats_df = analyze_image_properties(dataset_path, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sample-visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_images(dataset_path, class_names, samples_per_class=3):\n",
    "    \"\"\"\n",
    "    Visualize sample images from each class\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    fig, axes = plt.subplots(len(class_names), samples_per_class, \n",
    "                            figsize=(samples_per_class * 3, len(class_names) * 3))\n",
    "    \n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_dir = dataset_path / class_name\n",
    "        image_files = list(class_dir.glob('*.png'))[:samples_per_class]\n",
    "        \n",
    "        for j, img_path in enumerate(image_files):\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if len(class_names) == 1:\n",
    "                ax = axes[j]\n",
    "            else:\n",
    "                ax = axes[i, j]\n",
    "            \n",
    "            ax.imshow(img_rgb)\n",
    "            ax.set_title(f'{class_name}\\n{img_path.name}')\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples\n",
    "visualize_sample_images(dataset_path, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing-pipeline",
   "metadata": {},
   "source": [
    "## 5. Preprocessing Pipeline Design\n",
    "\n",
    "Based on our dataset exploration, let's design an appropriate preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path, img_size=(224, 224), test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Load and preprocess the entire dataset\n",
    "    \"\"\"\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Load all images\n",
    "    for class_dir in dataset_path.iterdir():\n",
    "        if class_dir.is_dir() and class_dir.name != '.DS_Store':\n",
    "            class_name = class_dir.name\n",
    "            \n",
    "            for img_path in class_dir.glob('*.png'):\n",
    "                # Load and preprocess image\n",
    "                img = cv2.imread(str(img_path))\n",
    "                if img is not None:\n",
    "                    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                    img_resized = cv2.resize(img_rgb, img_size)\n",
    "                    img_normalized = img_resized.astype(np.float32) / 255.0\n",
    "                    \n",
    "                    images.append(img_normalized)\n",
    "                    labels.append(class_name)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Split data\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y_encoded, test_size=test_size, stratify=y_encoded, random_state=42\n",
    "    )\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=val_size/(1-test_size), stratify=y_temp, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset loaded successfully!\")\n",
    "    print(f\"Training set: {X_train.shape[0]} images\")\n",
    "    print(f\"Validation set: {X_val.shape[0]} images\")\n",
    "    print(f\"Test set: {X_test.shape[0]} images\")\n",
    "    print(f\"Image shape: {X_train.shape[1:]}\")\n",
    "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), label_encoder\n",
    "\n",
    "# Load the dataset\n",
    "(X_train, X_val, X_test, y_train, y_val, y_test), label_encoder = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-augmentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# No augmentation for validation/test\n",
    "val_datagen = ImageDataGenerator()\n",
    "\n",
    "# Demonstrate data augmentation\n",
    "def show_augmentation_examples(image, datagen, num_examples=5):\n",
    "    \"\"\"\n",
    "    Show examples of data augmentation\n",
    "    \"\"\"\n",
    "    # Reshape for generator\n",
    "    image_batch = image.reshape((1,) + image.shape)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, num_examples + 1, figsize=(15, 3))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title('Original')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Augmented images\n",
    "    i = 1\n",
    "    for batch in datagen.flow(image_batch, batch_size=1):\n",
    "        axes[i].imshow(batch[0])\n",
    "        axes[i].set_title(f'Augmented {i}')\n",
    "        axes[i].axis('off')\n",
    "        i += 1\n",
    "        if i > num_examples:\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show augmentation examples\n",
    "sample_image = X_train[0]\n",
    "show_augmentation_examples(sample_image, train_datagen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cnn-model",
   "metadata": {},
   "source": [
    "## 6. Building Your First CNN\n",
    "\n",
    "Let's build a Convolutional Neural Network for our grapevine leaf classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-cnn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_cnn(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a simple CNN architecture\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        # First convolutional block\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second convolutional block\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third convolutional block\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Flatten and dense layers\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "input_shape = X_train.shape[1:]\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "model = create_simple_cnn(input_shape, num_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compile-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "print(\"Model compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "history = model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation metrics\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evaluation",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "evaluate-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_classes, \n",
    "                          target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_classes)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prediction-examples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_prediction_examples(model, X_test, y_test, label_encoder, num_examples=8):\n",
    "    \"\"\"\n",
    "    Show prediction examples with confidence scores\n",
    "    \"\"\"\n",
    "    # Get random samples\n",
    "    indices = np.random.choice(len(X_test), num_examples, replace=False)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test[indices])\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    confidence_scores = np.max(predictions, axis=1)\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        # Display image\n",
    "        axes[i].imshow(X_test[idx])\n",
    "        \n",
    "        # Get labels\n",
    "        true_label = label_encoder.classes_[y_test[idx]]\n",
    "        pred_label = label_encoder.classes_[predicted_classes[i]]\n",
    "        confidence = confidence_scores[i]\n",
    "        \n",
    "        # Set title with color coding\n",
    "        color = 'green' if true_label == pred_label else 'red'\n",
    "        axes[i].set_title(f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.2f}',\n",
    "                         color=color)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show prediction examples\n",
    "show_prediction_examples(model, X_test, y_test, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transfer-learning",
   "metadata": {},
   "source": [
    "## 8. Transfer Learning (Bonus)\n",
    "\n",
    "Let's also try transfer learning with a pre-trained MobileNetV2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transfer-learning-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transfer_learning_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create a transfer learning model using MobileNetV2\n",
    "    \"\"\"\n",
    "    # Load pre-trained MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add custom classifier\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create transfer learning model\n",
    "transfer_model = create_transfer_learning_model(input_shape, num_classes)\n",
    "transfer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "train-transfer-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile transfer learning model\n",
    "transfer_model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train transfer learning model\n",
    "transfer_history = transfer_model.fit(\n",
    "    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),\n",
    "    steps_per_epoch=len(X_train) // BATCH_SIZE,\n",
    "    epochs=20,  # Fewer epochs for transfer learning\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot transfer learning history\n",
    "plot_training_history(transfer_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare-models",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Simple CNN\n",
    "cnn_test_loss, cnn_test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Simple CNN - Test Accuracy: {cnn_test_accuracy:.4f}\")\n",
    "\n",
    "# Transfer Learning\n",
    "transfer_test_loss, transfer_test_accuracy = transfer_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Transfer Learning - Test Accuracy: {transfer_test_accuracy:.4f}\")\n",
    "\n",
    "print(f\"\\nImprovement: {transfer_test_accuracy - cnn_test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## 9. Next Steps & Deliverables\n",
    "\n",
    "### This Week's Deliverable: Image preprocessing + dataset setup\n",
    "\n",
    "**What you've accomplished:**\n",
    "1. ✅ Learned neural network fundamentals\n",
    "2. ✅ Mastered OpenCV basics for image preprocessing\n",
    "3. ✅ Conducted systematic dataset exploration\n",
    "4. ✅ Designed preprocessing pipelines\n",
    "5. ✅ Built and trained your first CNN\n",
    "6. ✅ Implemented transfer learning\n",
    "\n",
    "### For Saturday's Presentation: \"Dataset Insights + Preprocessing Plan\"\n",
    "\n",
    "**Prepare to present:**\n",
    "1. Dataset characteristics and class distribution\n",
    "2. Image properties analysis (dimensions, file sizes, etc.)\n",
    "3. Preprocessing pipeline design decisions\n",
    "4. Data augmentation strategy\n",
    "5. Initial model results and insights\n",
    "\n",
    "### Self-study: TensorFlow basics, image augmentation practice\n",
    "\n",
    "**Recommended activities:**\n",
    "1. Experiment with different augmentation parameters\n",
    "2. Try different CNN architectures\n",
    "3. Explore other pre-trained models\n",
    "4. Practice with TensorFlow/Keras documentation\n",
    "\n",
    "### Week 5 Preview: Advanced CNN Architectures\n",
    "- ResNet, DenseNet, EfficientNet\n",
    "- Advanced training techniques\n",
    "- Model optimization and deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exercises",
   "metadata": {},
   "source": [
    "## 10. Practice Exercises\n",
    "\n",
    "### Exercise 1: Modify the CNN Architecture\n",
    "Try adding more convolutional layers or changing filter sizes. How does it affect performance?\n",
    "\n",
    "### Exercise 2: Experiment with Data Augmentation\n",
    "Modify the augmentation parameters. What happens if you increase/decrease rotation_range?\n",
    "\n",
    "### Exercise 3: Feature Visualization\n",
    "Try to visualize what the convolutional layers are learning using activation maps.\n",
    "\n",
    "### Exercise 4: Different Optimizers\n",
    "Compare Adam, SGD, and RMSprop optimizers. Which works best for this dataset?\n",
    "\n",
    "### Exercise 5: Class Imbalance\n",
    "What would you do if one class had significantly fewer images? Research and implement solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inhibitor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
